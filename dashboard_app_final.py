# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
from scipy.signal import butter, lfilter
from scipy.stats import skew, kurtosis, entropy
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import IsolationForest
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ ØµÙØ­Ù‡
st.set_page_config(
    page_title="Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ù¾Ø§ÛŒØ´ Ø³Ù„Ø§Ù…Øª Ø³Ø§Ø²Ù‡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡
DATA_FOLDER = 'data'
TEST_FOLDER = 'test_data'

# -----------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ SHM3.py (Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡)
# -----------------------------------------------------------

def process_raw_data(folder_path):
    all_raw_data = pd.DataFrame()
    day_folders = sorted(glob.glob(os.path.join(folder_path, '*/')))
    if not day_folders:
        st.warning(f"Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯Ø± Ù¾ÙˆØ´Ù‡ '{folder_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return None
    for day_folder in day_folders:
        file_path = os.path.join(day_folder, 'Accelerometer.csv')
        date = os.path.basename(os.path.normpath(day_folder))
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)
                if {'x', 'y', 'z'}.issubset(df.columns):
                    df_accel = df[['x', 'y', 'z']].copy()
                    df_accel['date'] = date
                    all_raw_data = pd.concat([all_raw_data, df_accel], ignore_index=True)
            except Exception:
                continue
    return all_raw_data

def compute_magnitude(df_raw):
    df_magnitude = pd.DataFrame()
    if df_raw is None or df_raw.empty: return None
    for date, group in df_raw.groupby('date'):
        group['magnitude'] = np.sqrt(group['x']**2 + group['y']**2 + group['z']**2)
        df_magnitude = pd.concat([df_magnitude, group[['date', 'magnitude']]], ignore_index=True)
    return df_magnitude

def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    y = lfilter(b, a, data)
    return y

def apply_filter(df_magnitude, lowcut=0.1, highcut=10, fs=100):
    df_filtered = pd.DataFrame()
    if df_magnitude is None or df_magnitude.empty: return None
    for date, group in df_magnitude.groupby('date'):
        magnitude_filtered = butter_bandpass_filter(group['magnitude'].values, lowcut, highcut, fs)
        group['filtered_magnitude'] = magnitude_filtered
        df_filtered = pd.concat([df_filtered, group[['date', 'filtered_magnitude']]], ignore_index=True)
    return df_filtered

def extract_features(df_filtered):
    all_features = pd.DataFrame()
    if df_filtered is None or df_filtered.empty: return None
    for date, group in df_filtered.groupby('date'):
        signal = group['filtered_magnitude'].values
        features = {}
        features['mean'] = np.mean(signal)
        features['std'] = np.std(signal)
        features['variance'] = np.var(signal)
        features['max'] = np.max(signal)
        features['min'] = np.min(signal)
        features['median'] = np.median(signal)
        features['skewness'] = skew(signal)
        features['kurtosis'] = kurtosis(signal)
        N = len(signal)
        yf = np.fft.fft(signal)
        xf = np.fft.fftfreq(N, 1/100)
        psd = np.abs(yf[1:N//2])**2
        features['psd_mean'] = np.mean(psd)
        dominant_indices = np.argsort(psd)[-3:]
        dominant_freqs = xf[1:N//2][dominant_indices]
        for i, freq in enumerate(dominant_freqs):
            features[f'dom_freq_{i+1}'] = freq
        features['rms'] = np.sqrt(np.mean(signal**2))
        features['peak_to_peak'] = np.ptp(signal)
        hist, _ = np.histogram(signal, bins=100)
        hist_norm = hist / hist.sum()
        features['entropy'] = entropy(hist_norm, base=2)
        df_row = pd.DataFrame([features])
        df_row['date'] = date
        all_features = pd.concat([all_features, df_row], ignore_index=True)
    return all_features.set_index('date')

def mahalanobis_distance(x, mean, cov_inv):
    try:
        return np.sqrt(np.dot(np.dot((x - mean).values.reshape(1, -1), cov_inv), (x - mean).values.reshape(-1, 1)))[0][0]
    except (ValueError, np.linalg.LinAlgError):
        return np.inf

def build_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    encoder = Dense(64, activation='relu')(input_layer)
    encoder = Dense(32, activation='relu')(encoder)
    decoder = Dense(64, activation='relu')(encoder)
    output_layer = Dense(input_dim, activation='sigmoid')(decoder)
    autoencoder = Model(inputs=input_layer, outputs=output_layer)
    autoencoder.compile(optimizer='adam', loss='mse')
    return autoencoder

@st.cache_data
def run_analysis():
    st.info("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§. Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯...")
    df_raw = process_raw_data(DATA_FOLDER)
    if df_raw is None or df_raw.empty: return None, "Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯."

    df_magnitude = compute_magnitude(df_raw)
    df_filtered = apply_filter(df_magnitude)
    df_features = extract_features(df_filtered)
    if df_features is None or df_features.empty: return None, "Ù‡ÛŒÚ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒØ§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯."

    scaler = MinMaxScaler()
    df_normalized = pd.DataFrame(scaler.fit_transform(df_features), columns=df_features.columns, index=df_features.index)

    models = {}
    model_if = IsolationForest(random_state=42)
    model_if.fit(df_normalized)
    models['if'] = model_if
    
    mean_vec = df_normalized.mean()
    cov_matrix = df_normalized.cov()
    try:
        cov_inv = np.linalg.inv(cov_matrix)
        models['mahala'] = {'mean': mean_vec, 'cov_inv': cov_inv}
    except np.linalg.LinAlgError:
        pass
    
    input_dim = df_normalized.shape[1]
    model_ae = build_autoencoder(input_dim)
    model_ae.fit(df_normalized, df_normalized, epochs=50, batch_size=32, shuffle=True, validation_split=0.1, verbose=0)
    models['ae'] = model_ae
    
    return {
        'df_raw': df_raw,
        'df_magnitude': df_magnitude,
        'df_filtered': df_filtered,
        'df_features': df_features,
        'df_normalized': df_normalized,
        'models': models,
        'scaler': scaler
    }, None

def run_test_and_get_results(models, scaler, df_features_train):
    day_folders = sorted(glob.glob(os.path.join(TEST_FOLDER, '*/')))
    if not day_folders:
        return pd.DataFrame(), pd.DataFrame(), "Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª ÛŒØ§ÙØª Ù†Ø´Ø¯."
    
    test_results_df = pd.DataFrame(columns=['date', 'true_label', 'Isolation_Forest', 'Mahalanobis', 'Autoencoder', 'reason'])
    test_data = pd.DataFrame()

    for day_folder in day_folders:
        file_path = os.path.join(day_folder, 'Accelerometer.csv')
        date = os.path.basename(os.path.normpath(day_folder))
        if not os.path.exists(file_path): continue
        try:
            true_label = 1 if ('frequency_shift' in date or 'spike' in date) else 0
            df_raw_test = pd.read_csv(file_path)
            if not {'x', 'y', 'z'}.issubset(df_raw_test.columns): continue
            
            df_raw_test['magnitude'] = np.sqrt(df_raw_test['x']**2 + df_raw_test['y']**2 + df_raw_test['z']**2)
            df_raw_test['filtered_magnitude'] = butter_bandpass_filter(df_raw_test['magnitude'].values, 0.1, 10, 100)
            df_raw_test['date'] = date
            test_data = pd.concat([test_data, df_raw_test], ignore_index=True)

            test_features = extract_features(df_raw_test[['filtered_magnitude', 'magnitude', 'date']].dropna())
            if test_features is None or test_features.empty: continue
            
            df_normalized_test = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns)

            if_prediction = 1 if models['if'].predict(df_normalized_test)[0] == -1 else 0
            mahala_prediction = 0
            if 'mahala' in models:
                mahala_score = mahalanobis_distance(df_normalized_test.iloc[0], models['mahala']['mean'], models['mahala']['cov_inv'])
                mahala_prediction = 1 if mahala_score > 1.0 else 0

            ae_prediction = 0
            if 'ae' in models:
                ae_reconstruction_error = np.mean(np.power(df_normalized_test - models['ae'].predict(df_normalized_test, verbose=0), 2), axis=1)[0]
                ae_prediction = 1 if ae_reconstruction_error > 0.5 else 0

            anomaly_reason = ""
            if if_prediction == 1 or mahala_prediction == 1 or ae_prediction == 1:
                df_features_train_scaled = pd.DataFrame(scaler.transform(df_features_train), columns=df_features_train.columns)
                mean_normal = df_features_train_scaled.mean()
                std_normal = df_features_train_scaled.std()
                deviations = np.abs(df_normalized_test.iloc[0] - mean_normal) / std_normal
                top_deviated_features = deviations.nlargest(3)
                reason_parts = [f"{feature}: {test_features[feature].iloc[0]:.2f}" for feature, _ in top_deviated_features.items()]
                anomaly_reason = " | ".join(reason_parts)
            
            new_row = pd.DataFrame([{
                'date': date,
                'true_label': true_label,
                'Isolation_Forest': if_prediction,
                'Mahalanobis': mahala_prediction,
                'Autoencoder': ae_prediction,
                'reason': anomaly_reason
            }])
            test_results_df = pd.concat([test_results_df, new_row], ignore_index=True)
        except Exception:
            continue
    return test_results_df, test_data, None

# -----------------------------------------------------------
# Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Streamlit
# -----------------------------------------------------------

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
analysis_results, analysis_error = run_analysis()

# Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ
st.title("Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ù¾Ø§ÛŒØ´ Ø³Ù„Ø§Ù…Øª Ø³Ø§Ø²Ù‡ Ù¾Ù„ Ú©Ø§Ø¨Ù„ÛŒ ØªØ¨Ø±ÛŒØ²")
st.markdown("---")

if analysis_error:
    st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {analysis_error}")
    st.stop()
    
# ØªØ³Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
test_results_df, test_data, test_error = run_test_and_get_results(analysis_results['models'], analysis_results['scaler'], analysis_results['df_features'])
if test_error:
    st.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª: {test_error}")
    st.stop()

# Ø¨Ø®Ø´ Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª ØªØ­Ù„ÛŒÙ„
st.header("Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª ØªØ­Ù„ÛŒÙ„")
st.markdown("**Ù†ØªØ§ÛŒØ¬ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯:**")
if not test_results_df.empty:
    st.dataframe(test_results_df)
else:
    st.info("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ ØªØ³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")

# Ø¨Ø®Ø´ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ùˆ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
st.markdown("---")
st.header("Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ùˆ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡")

tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…",
    "ÙÛŒÙ„ØªØ± Ùˆ Ø¯Ø§Ù…Ù†Ù‡ Ø´ØªØ§Ø¨",
    "ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§",
    "ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª",
    "Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§",
    "Ú¯Ø²Ø§Ø±Ø´ ØªÙØ³ÛŒØ±ÛŒ"
])

# ØªØ¨ Û±: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…
with tab1:
    st.subheader("Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…")
    df_raw = analysis_results['df_raw']
    if df_raw is not None and not df_raw.empty:
        for date, group in df_raw.groupby('date'):
            st.markdown(f"**Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² {date}:**")
            fig, ax = plt.subplots(figsize=(12, 6))
            ax.plot(group['x'], label='X-axis')
            ax.plot(group['y'], label='Y-axis')
            ax.plot(group['z'], label='Z-axis')
            ax.set_xlabel('Sample Index')
            ax.set_ylabel('Acceleration')
            ax.set_title(f'Raw Accelerometer Data - {date}')
            ax.legend()
            st.pyplot(fig)
            plt.close(fig)
            st.markdown("---")
    else:
        st.info("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# ØªØ¨ Û²: ÙÛŒÙ„ØªØ± Ùˆ Ø¯Ø§Ù…Ù†Ù‡ Ø´ØªØ§Ø¨
with tab2:
    st.subheader("Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ùˆ Ø¯Ø§Ù…Ù†Ù‡ Ø´ØªØ§Ø¨")
    df_magnitude = analysis_results['df_magnitude']
    df_filtered = analysis_results['df_filtered']
    
    if df_magnitude is not None and not df_magnitude.empty:
        st.markdown("**Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ù…Ù†Ù‡ Ø´ØªØ§Ø¨:**")
        for date, group in df_magnitude.groupby('date'):
            fig, ax = plt.subplots()
            ax.plot(group['magnitude'])
            ax.set_title(f'Accelerometer Magnitude - {date}')
            ax.set_xlabel('Sample Index')
            ax.set_ylabel('Magnitude')
            st.pyplot(fig)
            plt.close(fig)
            st.markdown("---")
    
    if df_filtered is not None and not df_filtered.empty:
        st.markdown("**Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ÙÛŒÙ„ØªØ±Ø´Ø¯Ù‡:**")
        for date, group in df_filtered.groupby('date'):
            fig, ax = plt.subplots()
            raw_mag = df_magnitude[df_magnitude['date'] == date]['magnitude'].values
            ax.plot(raw_mag, label='Raw Magnitude')
            ax.plot(group['filtered_magnitude'], label='Filtered Magnitude')
            ax.set_title(f'Raw vs Filtered Magnitude - {date}')
            ax.set_xlabel('Sample Index')
            ax.set_ylabel('Magnitude')
            ax.legend()
            st.pyplot(fig)
            plt.close(fig)
            st.markdown("---")
    if (df_magnitude is None or df_magnitude.empty) and (df_filtered is None or df_filtered.empty):
        st.info("Ù‡ÛŒÚ† Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§Ù…Ù†Ù‡ ÛŒØ§ ÙÛŒÙ„ØªØ±Ø´Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# ØªØ¨ Û³: ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
with tab3:
    st.subheader("ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§")
    df_features = analysis_results['df_features']
    if df_features is not None and not df_features.empty:
        st.markdown("**Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:**")
        fig, ax = plt.subplots(figsize=(12, 10))
        sns.heatmap(df_features.corr(), annot=True, fmt=".2f", cmap='coolwarm', ax=ax)
        st.pyplot(fig)
        plt.close(fig)
        st.info("Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Û± ÛŒØ§ -Û± Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù‚ÙˆÛŒ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§Ø³Øª.")
    else:
        st.info("Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# ØªØ¨ Û´: ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª
with tab4:
    st.subheader("ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§")
    df_normalized = analysis_results['df_normalized']
    models = analysis_results['models']
    if df_normalized is not None and not df_normalized.empty and models:
        thresholds = np.arange(0.80, 1.0, 0.01)
        results = {'if': [], 'mahala': [], 'ae': []}
        
        if 'if' in models:
            if_scores = models['if'].decision_function(df_normalized)
            for threshold in thresholds:
                if_anomalies = np.sum(if_scores < np.percentile(if_scores, (1-threshold)*100))
                results['if'].append(if_anomalies)
        
        if 'mahala' in models:
            mahala_scores = df_normalized.apply(lambda row: mahalanobis_distance(row, models['mahala']['mean'], models['mahala']['cov_inv']), axis=1)
            for threshold in thresholds:
                mahala_anomalies = np.sum(mahala_scores > np.percentile(mahala_scores, threshold*100))
                results['mahala'].append(mahala_anomalies)
        
        if 'ae' in models:
            ae_reconstruction_error = np.mean(np.power(df_normalized - models['ae'].predict(df_normalized, verbose=0), 2), axis=1)
            for threshold in thresholds:
                ae_anomalies = np.sum(ae_reconstruction_error > np.percentile(ae_reconstruction_error, threshold*100))
                results['ae'].append(ae_anomalies)
                
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.set_title('Sensitivity Analysis - Number of Anomalies vs Threshold', fontsize=16)
        if 'if' in results:
            ax.plot(thresholds, results['if'], label='Isolation Forest')
        if 'mahala' in results:
            ax.plot(thresholds, results['mahala'], label='Mahalanobis Distance')
        if 'ae' in results:
            ax.plot(thresholds, results['ae'], label='Autoencoder')
        ax.set_xlabel('Threshold (%)')
        ax.set_ylabel('Number of Detected Anomalies')
        ax.legend()
        st.pyplot(fig)
        plt.close(fig)
        
        st.markdown("**Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª:**")
        st.info("Ø§ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¨Ø§ ØªØºÛŒÛŒØ± Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ø² 80% ØªØ§ 99%ØŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù‡Ø± Ù…Ø¯Ù„ Ú†Ú¯ÙˆÙ†Ù‡ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.")
        report_df = pd.DataFrame(results, index=[f'{int(t*100)}%' for t in thresholds])
        st.dataframe(report_df)
    else:
        st.info("ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ù†ÛŒØ³Øª. Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÛŒØ§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³ØªÙ†Ø¯.")

# ØªØ¨ Ûµ: Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§
with tab5:
    st.subheader("Ù†ØªØ§ÛŒØ¬ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ")
    if not test_results_df.empty:
        for index, row in test_results_df.iterrows():
            date = row['date']
            test_data_group = test_data[test_data['date'] == date]
            if not test_data_group.empty:
                st.markdown(f"**Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ {date}:**")
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(test_data_group['filtered_magnitude'], label='Filtered Data')
                ax.text(0.05, 0.95, f'True Label: {"Anomaly" if row["true_label"] == 1 else "Normal"}', 
                         transform=ax.transAxes, fontsize=12, color='black', weight='bold')
                if row['Isolation_Forest'] == 1:
                    ax.scatter(np.arange(len(test_data_group))[::2000], test_data_group['filtered_magnitude'].values[::2000], color='red', s=100, label='IF Anomaly', marker='X')
                if row['Mahalanobis'] == 1:
                    ax.scatter(np.arange(len(test_data_group))[::2000], test_data_group['filtered_magnitude'].values[::2000], color='green', s=100, label='Mahalanobis Anomaly', marker='X')
                if row['Autoencoder'] == 1:
                    ax.scatter(np.arange(len(test_data_group))[::2000], test_data_group['filtered_magnitude'].values[::2000], color='purple', s=100, label='AE Anomaly', marker='X')
                ax.set_xlabel('Sample Index')
                ax.set_ylabel('Magnitude')
                ax.legend()
                st.pyplot(fig)
                plt.close(fig)
                st.markdown("---")
    else:
        st.info("Ù‡ÛŒÚ† Ù†Ù…ÙˆØ¯Ø§Ø± Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")

# ØªØ¨ Û¶: Ú¯Ø²Ø§Ø±Ø´ ØªÙØ³ÛŒØ±ÛŒ
with tab6:
    st.subheader("Ú¯Ø²Ø§Ø±Ø´ ØªÙØ³ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ")
    if not test_results_df.empty:
        st.markdown("**Ø®Ù„Ø§ØµÙ‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§**")
        st.dataframe(test_results_df)

        st.markdown("---")
        st.markdown("**ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡** ğŸ•µï¸â€â™‚ï¸")
        anomaly_dates = test_results_df[test_results_df['true_label'] == 1]
        if not anomaly_dates.empty:
            for index, row in anomaly_dates.iterrows():
                st.info(f"**ØªØ§Ø±ÛŒØ®:** {row['date']}")
                st.markdown(f"**Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ:** {'Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ' if row['true_label'] == 1 else 'Ø¹Ø§Ø¯ÛŒ'}")
                st.markdown(f"**Ø¯Ù„Ø§ÛŒÙ„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªÙˆØ³Ø· Ù…Ø¯Ù„â€ŒÙ‡Ø§:** {row['reason']}")
                st.markdown("---")
        else:
            st.info("Ù‡ÛŒÚ† Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª ÛŒØ§ÙØª Ù†Ø´Ø¯.")

    else:
        st.info("Ú¯Ø²Ø§Ø±Ø´ ØªÙØ³ÛŒØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ú©Ø¯ ØªØ­Ù„ÛŒÙ„ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")

# -----------------------------------------------------------
# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡
# -----------------------------------------------------------
st.sidebar.title("Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡")
st.sidebar.info(
    "1. Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ ØªØ­Ù„ÛŒÙ„ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n"
    "2. ØªÙ…Ø§Ù… Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ `data` Ùˆ `test_data` Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ù‡Ù…Ø§Ù† Ù…Ø®Ø²Ù† Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯.\n"
    "3. Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÙˆÚ©Ø§Ù„ØŒ Ø§Ø² Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:\n\n"
    "`streamlit run dashboard_final.py`"
)
st.sidebar.markdown("---")
st.sidebar.success("Ø¨Ø§ Ø§ÛŒÙ† Ú©Ø§Ø±ØŒ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Ø´Ù…Ø§ Ø¨Ø§Ø² Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.")
